import json
from typing import List, Dict, Tuple
from datasets import load_dataset

import os
import subprocess
from tqdm import tqdm
import numpy as np
import time
import argparse
import random
from multiprocessing import Pool, Manager, Queue
from datetime import datetime

from models.data.utils import generate_random_split, load_leetcode_dataset
from utils import (get_model_name,
                   make_task_name,
                   GENERATED_CODE_DIR,
                   SPLIT_SYM,
                   PASS_AT_K_DIR,
                   EXE_RES_DIR)

PY_BIN = "{INSERT PY BIN}"
random.seed(65)

def estimator(n: int, c: int, k: int) -> float:
    """
    Pass@k
    Calculates 1 - comb(n - c, k) / comb(n, k).
    """
    if n - c < k:
        return 1.0
    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))

def execute_script(args):
    """
    Execute python code using subprocess procedure
    """
    file_path, subdir_name = args
    try:
        result = subprocess.run(
            [PY_BIN, file_path],
            capture_output=True,
            text=True,
            timeout=1  # Timeout to prevent infinite loops
        )
        return file_path, subdir_name, result.returncode == 0
    except Exception:
        return file_path, subdir_name, False

def exec_code(task_dir):
    """
    Executes code generated by LLM
    """

    # initializes time and results dictionary
    t1 = time.time()
    execution_results = {}

    tasks = []

    # Traverse through directory containing evaluation results
    for root, dirs, files in os.walk(task_dir):
        subdir_name = os.path.basename(root)
        # Find all the python files - these are the code generated files
        # Append all these file paths to tasks dictionary
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                tasks.append((file_path, subdir_name))

    # Use multiprocessing Pool for parallel execution
    results_queue = Queue()
    # os.cpu_count()
    # execute python scripts and get results
    with Pool(processes=256) as pool:
        # Use map_async to avoid broken pipe issue, and get results through the queue
        async_result = pool.map_async(execute_script, tasks)

        # Collect results from the async result
        for result in async_result.get():
            results_queue.put(result)

    # Organize results into the dictionary format
    while not results_queue.empty():
        file_path, subdir_name, success = results_queue.get()
        file_name = os.path.basename(file_path)
        if file_name not in execution_results:
            execution_results[file_name] = {}
        execution_results[file_name][subdir_name] = success

    t2 = time.time()
    print(f"Execution time: {t2 - t1} seconds")


    return execution_results

def eval_one_dir(eval_dir):
    model_name, data_name, hyper_params = eval_dir.split('/')[-3:]
    task_name = f"{model_name}{SPLIT_SYM}{data_name}{SPLIT_SYM}{hyper_params}"
    exe_res_path = os.path.join(EXE_RES_DIR, f"{task_name}.json")
    pass_k_path = os.path.join(PASS_AT_K_DIR, f"{task_name}.json")

    execution_results = exec_code(eval_dir)

    pass_k = {}
    c_count = 0
    for name, data in execution_results.items():
        n = len(data)
        c = len([d for d in data.values() if d is True])
        c_count += c
        pass_k[name] = {
            "pass@1": estimator(n, c, 1),
            "pass@10": estimator(n, c, 10),
            "pass@100": estimator(n, c, 100),
            "n": n}
        # Save results to JSON

    with open(exe_res_path, "w") as json_file:
        json.dump(execution_results, json_file, indent=4)

    with open(pass_k_path, "w") as json_file:
        json.dump(pass_k, json_file, indent=4)

    pass_1 = np.mean([r["pass@1"] for r in pass_k.values()])
    pass_10 = np.mean([r["pass@10"] for r in pass_k.values()])
    pass_100 = np.mean([r["pass@100"] for r in pass_k.values()])

    return {
        "pass_1": pass_1,
        "pass_10": pass_10,
        "pass_100": pass_100,
    }

def eval_task(task_dir):
    all_res = {}
    for hyper_name in os.listdir(task_dir):
        eval_dir = os.path.join(task_dir, hyper_name)
        pass_k = eval_one_dir(eval_dir)
        print(task_dir, hyper_name)
        for k in pass_k.keys():
            print(k, pass_k[k])
        all_res[hyper_name] = pass_k
    return all_res


def main(args):
    model_name = get_model_name(args.model_id)
    task_name = make_task_name(model_name, None, None)

    # POST LEETCODE EVALUATION
    if args.pre == 1:
        model_date = datetime.strptime("2024-09-19", "%Y-%m-%d").date()
        task_dir = os.path.join(
            GENERATED_CODE_DIR, task_name, "LeetCodePost",
        )
        for hyper_name in os.listdir(task_dir):
            eval_dir = os.path.join(task_dir, hyper_name)
            model_name, data_name, hyper_params = eval_dir.split('/')[-3:]
            task_name = f"{model_name}{SPLIT_SYM}{data_name}{SPLIT_SYM}{hyper_params}"
            print(task_name)
            pass_k_path = os.path.join(PASS_AT_K_DIR, f"{task_name}.json")

            with open(pass_k_path, "r") as json_file:
                res = json.load(json_file)

            problems = load_leetcode_dataset("models/data/leetcode/LeetCodeDataset-v0.3.1-test.jsonl")

            easy = []
            med = []
            hard = []
            for problem in problems:
                if datetime.strptime(problem["estimated_date"], "%Y-%m-%d").date() > model_date:
                    diff = problem["difficulty"]
                    if diff == 'Easy':
                        easy.append(problem)
                    elif diff == 'Medium':
                        med.append(problem)
                    else:
                        hard.append(problem)

            easy = random.sample(easy, 40)
            med = random.sample(med, 40)
            hard = random.sample(hard, 40)
            sampled_problems = easy + med + hard

            num_correct = 0
            num_count = 120
            for problem in sampled_problems:
                problem_name = "code_" + problem["task_id"] + ".py"
                result = res[problem_name]["pass@1"]
                num_correct += result

            acc = num_correct / num_count
            print(f"Average accuracy is {acc}")
            return

    task_dir = os.path.join(
        GENERATED_CODE_DIR, task_name, "LeetCode",
    )
    for hyper_name in os.listdir(task_dir):
        num_splits = 100
        eval_dir = os.path.join(task_dir, hyper_name)
        model_name, data_name, hyper_params = eval_dir.split('/')[-3:]
        task_name = f"{model_name}{SPLIT_SYM}{data_name}{SPLIT_SYM}{hyper_params}"
        print(task_name)
        pass_k_path = os.path.join(PASS_AT_K_DIR, f"{task_name}.json")

        with open(pass_k_path, "r") as json_file:
            res = json.load(json_file)

        problems = load_leetcode_dataset("models/data/leetcode/LeetCodeDataset-v0.3.1-train.jsonl")
        splits = generate_random_split(num_splits)
        accs = []

        # for each split, find accuracy before and after
        for split in splits:

            # create pre and post split problem sets
            pre_easy = []
            pre_med = []
            pre_hard = []
            post_easy = []
            post_med = []
            post_hard = []
            for problem in problems:
                problem_date = datetime.strptime(problem["estimated_date"], "%Y-%m-%d")
                diff = problem["difficulty"]

                if problem_date < split:
                    if diff == 'Easy':
                        pre_easy.append(problem)
                    elif diff == 'Medium':
                        pre_med.append(problem)
                    else:
                        pre_hard.append(problem)
                else:
                    if diff == 'Easy':
                        post_easy.append(problem)
                    elif diff == 'Medium':
                        post_med.append(problem)
                    else:
                        post_hard.append(problem)

            # Sample 50 from each dataset
            pre_easy = random.sample(pre_easy, 50)
            pre_med = random.sample(pre_med, 50)
            pre_hard = random.sample(pre_hard, 50)
            post_easy = random.sample(post_easy, 50)
            post_med = random.sample(post_med, 50)
            post_hard = random.sample(post_hard, 50)

            pre_problems = pre_easy + pre_med + pre_hard
            post_problems = post_easy + post_med + post_hard

            pre_num_correct = 0
            post_num_correct = 0
            pre_count = 150
            post_count = 150
            for problem in pre_problems:
                problem_name = "code_" + problem["task_id"] + ".py"
                result = res[problem_name]["pass@1"]
                pre_num_correct += result

            for problem in post_problems:
                problem_name = "code_" + problem["task_id"] + ".py"
                result = res[problem_name]["pass@1"]
                post_num_correct += result

            pre_acc = pre_num_correct / pre_count
            post_acc = post_num_correct / post_count
            accs.append((pre_acc, post_acc))

        avg_pre_acc = sum(i for i,j in accs) / num_splits
        avg_post_acc = sum(j for i, j in accs) / num_splits

        print(f"Average accuracy in Pre-split: {avg_pre_acc}")
        print(f"Average accuracy in Post-split: {avg_post_acc}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_id', type=int, default=6)
    parser.add_argument('--pre', type=int, default=1) # Pre=0, Post=1
    args = parser.parse_args()
    main(args)
